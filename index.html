<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="CLiFT: Compressive Light-Field Tokens for Compute Efficient and Adaptive Neural Rendering">
  <meta name="keywords" content="CLiFT: Compressive Light-Field Tokens for Compute Efficient and Adaptive Neural Rendering">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>CLiFT</title>

  <!--  Open Graph / Facebook -->
  <meta property="og:title" content="CLiFT: Compressive Light-Field Tokens for Compute-Efficient and Adaptive Neural Rendering" />
  <meta property="og:type" content="website" />
  <meta property="og:url" content="https://clift-nvs.github.io/" />
  <meta property="og:image" content="https://clift-nvs.github.io/resources/preview.png" />
  <meta property="og:description" content="A compressed light-field representation enabling adaptive 3D neural rendering with configurable trade-offs in speed, storage, and quality." />

  <!-- Twitter -->
  <meta name="twitter:card" content="summary_large_image">
  <meta name="twitter:title" content="CLiFT: Compressive Light-Field Tokens for Compute-Efficient and Adaptive Neural Rendering">
  <meta name="twitter:description" content="A compressed light-field representation enabling adaptive 3D neural rendering with configurable trade-offs in speed, storage, and quality.">
  <meta name="twitter:image" content="https://clift-nvs.github.io/resources/preview.png">

  <!-- Bootstrap -->
  <link rel="stylesheet" href="./static/css/bootstrap-4.4.1.css">

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-LYK5B1RWXZ"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-LYK5B1RWXZ');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">
  
  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="https://clift-nvs.github.io/resources/ice-cliff.png" type="image/png">
  

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
  <script src="./static/js/app.js"></script>
  <script src="./static/js/video_comparison.js"></script>

  <link rel="stylesheet" href="./static/css/dics.original.css">
  <script src="./static/js/event_handler.js"></script>
  <script src="./static/js/dics.original.js"></script>

</head>
<body>


<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h2 class="title is-2 publication-title" style="margin-bottom: 0">
            <img src="./resources/ice-cliff.svg" alt="CLiFT icon" style="width: 32px; height: 32px; vertical-align: middle; margin-right: 8px;">
            <strong>CLiFT: </strong> Compressive Light-Field Tokens for Compute-Efficient and Adaptive Neural Rendering
          </h2>
          <!-- <h2 class="title is-2 publication-title" style="margin-top: 0; margin-bottom: 0"></h2> -->
          <br>
          <br>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://eric-zqwang.github.io/">Zhengqing Wang</a><sup>1</sup></span>&nbsp;&nbsp;&nbsp;&nbsp;
            <span class="author-block">
              <a href="https://ivenwu.com/">Yuefan Wu</a><sup>1</sup></span>&nbsp;&nbsp;&nbsp;&nbsp;
            <span class="author-block">
              <a href="https://jcchen.me/">Jiacheng Chen</a><sup>1</sup>
            </span>&nbsp;&nbsp;&nbsp;&nbsp;
            <span class="author-block">
              <a href="https://zhangfuyang.github.io/">Fuyang Zhang</a><sup>1</sup>
            </span>&nbsp;&nbsp;&nbsp;&nbsp;
            <span class="author-block">
              <a href="https://yasu-furukawa.github.io/">Yasutaka Furukawa</a><sup>1,2</sup>
            </span>
          </div>

          <div class="is-size-5 publication-authors affiliations">
            <span class="author-block"><sup>1</sup><a href="https://gruvi.cs.sfu.ca/" class="affiliation-link">Simon Fraser University</a></span>&nbsp;&nbsp;&nbsp;&nbsp;
            <span class="author-block"><sup>2</sup><a href="https://wayve.ai/" class="affiliation-link">Wayve</a></span>&nbsp;&nbsp;&nbsp;&nbsp;
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a href="./resources/paper.pdf"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>

              <span class="link-block">
                <a href="https://arxiv.org/abs/2507.08776"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
          
              <!-- Code Link. -->
              <span class="link-block">
                <a href="https://github.com/eric-zqwang/CLiFT.git"
                   class="external-link button is-normal is-rounded is-dark disabled">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code (release by 8/1)</span>
                  </a>
              </span>

              <!-- Code Link. -->
            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <div class="columns is-centered has-text-centered">
        <div class="column is-full-width">
      
          <div>
            <video controls autoplay loop playsinline webkit-playsinline preload="metadata" x5-video-player-type="h5" x5-video-orientation="portraint" x5-video-player-fullscreen="true">
              <source src="resources/demo_with_audio.mp4" type="video/mp4">
              <source src="resources/demo_with_audio.webm" type="video/webm">
              Your browser does not support the video tag.
            </video>
          </div>
      
          <div class="tldr-caption">
            We represent a 3D scene as <strong>compressed light-field tokens (CLiFTs)</strong> that enable adaptive neural rendering with configurable compute budgets, providing flexible trade-offs between rendering speed, representation storage, and visual quality.
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3" style="margin-top: -30px">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            This paper proposes a neural rendering approach that represents a scene as "compressed light-field tokens (CLiFTs)", retaining rich appearance and geometric information of a scene. CLiFT enables compute-efficient rendering by compressed tokens, while being capable of changing the number of tokens to represent a scene or render a novel view with one trained network. Concretely, given a set of images, multi-view encoder tokenizes the images with the camera poses. Latent-space K-means selects a reduced set of rays as cluster centroids using the tokens. The multi-view "condenser" compresses the information of all the tokens into the centroid tokens to construct CLiFTs. At test time, given a target view and a compute budget (i.e., the number of CLiFTs), the system collects the specified number of nearby tokens and synthesizes a novel view using a compute-adaptive renderer. Extensive experiments on RealEstate10K and DL3DV datasets quantitatively and qualitatively validate our approach, achieving significant data reduction with comparable rendering quality and the highest overall rendering score, while providing trade-offs of data size, rendering quality, and rendering speed.
          </p>
        </div>
      </div>
    </div>
    
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">

    <!-- Method. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-full-width">
        <h2 class="title is-3" style="margin-top: -20px">Method Overview</h2>

        <figure class="media-container">
          <div>
            <figcaption class="method-caption">
              We present a step-by-step visualization of CLiFT's <strong>test-time process</strong>, demonstrating how multi-view images are <strong>tokenized</strong>, <strong>clustered</strong>, and <strong>compressed</strong> into CLiFTs, followed by the adaptive rendering process that selects relevant CLiFTs based on the target viewpoint and compute budget.
            </figcaption>
            <video controls autoplay muted loop playsinline webkit-playsinline preload="metadata" x5-video-player-type="h5" x5-video-orientation="portraint" x5-video-player-fullscreen="true">
              <source src="resources/dark_mode/intro_dark_compressed.mp4" type="video/mp4">
              <source src="resources/dark_mode/intro_dark_compressed.webm" type="video/webm">
              Your browser does not support the video tag.
           </video>
          </div>
        </figure>

        <div style="margin-top: 60px; margin-bottom: 40px;">
          <figure class="media-container">
            <figcaption class="method-caption">
              We then illustrate the detailed method pipeline. <strong>Training (top):</strong> Multi-view images are processed through three stages: (1) <em>Multi-view encoder</em> tokenizes input images with camera poses, (2) <a href="#latent-kmeans-vis"><em>Latent K-means</em></a> clusters tokens to select representative centroids, and (3) <em>Neural condensation</em> compresses all token information into the selected centroids to create CLiFTs. <strong>Inference (bottom):</strong> The multi-view input images are first encoded into CLiFTs by the same process as in training. Then, relevant CLiFTs are collected based on the target viewpoint and compute budget, which are passed to the adaptive renderer to produce the target view.
            </figcaption>
            <div class="content has-text-centered">
              <img src="resources/method_dark.png" alt="Method Overview">
            </div>
          </figure>
        </div>

      </div>
    </div>

  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">

    <!-- Results. -->
    <div class="columns is-centered ">
      <div class="column is-full-width">
        <h2 class="title is-3 has-text-centered" style="margin-top: -30px">Results</h2>

        <h3 class="title is-4 has-text-centered" style="margin-top: 40px;">Comparison wtih LVSM on RealEstate10K</h3>
        <div>
          <video autoplay loop playsinline muted x5-video-player-type="h5" x5-video-orientation="portraint" x5-video-player-fullscreen="true">
            <source src="resources/dark_mode/lvsm_comp_dark_compressed.mp4" type="video/mp4">
            <source src="resources/dark_mode/lvsm_comp_dark_compressed.webm" type="video/webm">
            Your browser does not support the video tag.
          </video>
        </div>


        <h3 class="title is-4 has-text-centered" style="margin-top: 40px;">Comparison wtih DepthSplat on DL3DV</h3>
        <!-- <div class="content has-text-justified">
          <p>
          </p>
        </div> -->
        
        <div>
          <video autoplay loop playsinline muted x5-video-player-type="h5" x5-video-orientation="portraint" x5-video-player-fullscreen="true">
            <source src="resources/dark_mode/depthsplat_comp_dark_compressed.mp4" type="video/mp4">
            <source src="resources/dark_mode/depthsplat_comp_dark_compressed.webm" type="video/webm">
            Your browser does not support the video tag.
          </video>
        </div>

        <h3 class="title is-4 has-text-centered" style="margin-top: 60px;">Detailed Comparison</h3>
        <div class="content has-text-justified">
          <p>
            Some detailed frame-level comparisons between DepthSplat and CLiFT with different compression ratios.
          </p>
        </div>

        <div class="container">
          <ul class="nav nav-tabs nav-fill nav-justified responsive-nav" id="object-scale-recon">
              <li class="nav-item">
                <a class="nav-link" onclick="dl3dvSceneEvent(0)">example 1</a>
              </li>
              <li class="nav-item">
                <a class="nav-link" onclick="dl3dvSceneEvent(1)">example 2</a>
              </li>
              <li class="nav-item">
                <a class="nav-link" onclick="dl3dvSceneEvent(2)">example 3</a>
              </li>
              <li class="nav-item">
                <a class="nav-link" onclick="dl3dvSceneEvent(3)">example 4</a>
              </li>
              <li class="nav-item">
                <a class="nav-link" onclick="dl3dvSceneEvent(4)">example 5</a>
              </li>
              <li class="nav-item">
                <a class="nav-link" onclick="dl3dvSceneEvent(5)">example 6</a>
              </li>
          </ul>
          <div class="b-dics" style="width: 100%; max-width: 900px; margin: 0 auto; font-weight: 600;">
              <img src="resources/360_stmt_clift/0a1b7c20a9_depthsplat.png" alt="DepthSplat (41 MB)">
              <img src="resources/360_stmt_clift/0a1b7c20a9_CLiFT25.png" alt="CLiFT (8 MB)">
              <img src="resources/360_stmt_clift/0a1b7c20a9_CLiFT50.png" alt="CLiFT (16 MB)">
              <img src="resources/360_stmt_clift/0a1b7c20a9_CLiFT100.png" alt="CLiFT (33 MB)">
              <img src="resources/360_stmt_clift/0a1b7c20a9_GT.png" alt="GT">
          </div>
        </div>

        <h3 class="title is-4 has-text-centered" style="margin-top: 60px;">More Qualitative Results on RealEstate10K</h3>
        <div class="content has-text-justified">
          <p>
          </p>
        </div>
        
        <div>
          <video autoplay loop playsinline muted x5-video-player-type="h5" x5-video-orientation="portraint" x5-video-player-fullscreen="true">
            <source src="resources/dark_mode/re10k_dark_compressed.mp4" type="video/mp4">
            <source src="resources/dark_mode/re10k_dark_compressed.webm" type="video/webm">
            Your browser does not support the video tag.
          </video>
        </div>

        <h3 class="title is-4 has-text-centered" style="margin-top: 60px;">More Qualitative Results on DL3DV</h3>
        <div class="content has-text-justified">
          <p>
          </p>
        </div>
        
        <div>
          <video autoplay loop playsinline muted x5-video-player-type="h5" x5-video-orientation="portraint" x5-video-player-fullscreen="true">
            <source src="resources/dark_mode/dl3dv_dark_compressed.mp4" type="video/mp4">
            <source src="resources/dark_mode/dl3dv_dark_compressed.webm" type="video/webm">
            Your browser does not support the video tag.
          </video>
        </div>

    </div>

  </div>
</section>

<section class="section" id="latent-kmeans-vis">
  <div class="container is-max-desktop">

    <div class="columns is-centered">
      <div class="column is-full-width has-text-centered">
        <h2 class="title is-3">Visualization of Latent K-means Clustering</h2>
        <figcaption class="method-caption">
          Finally, we visualize the latent K-means clustering results with K=128 for better visualization. Each color represents a cluster, and the yellow ring indicates the centroid token. Note that clustering is performed across multiple views, so a single cluster can span multiple images. As a result, some clusters may not have a visible centroid in a given image.
        </figcaption>
        <div class="content has-text-centered">
          <img src="resources/kmeans_vis.png" alt="K-means clustering visualization" style="margin-top: 20px;">
        </div>
      </div>
    </div>

  </div>
</section>

<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title has-text-centered">BibTeX</h2>
    <pre class="bibtex-container"><code>@article{Wang2025CLiFT,
  author    = {Wang, Zhengqing and Wu, Yuefan and Chen, Jiacheng and Zhang, Fuyang and Furukawa, Yasutaka},
  title     = {CLiFT: Compressive Light-Field Tokens for Compute Efficient and Adaptive Neural Rendering},
  journal   = {arXiv preprint arXiv:2507.08776},
  year      = {2025},
}</code></pre>
  </div>
</section>

<section class="section" id="Acknowledgements">
  <div class="container is-max-desktop content">
    <h2 class="title">Acknowledgements</h2>
    We thank Haian Jin for helpful discussions on reproducing LVSM and training on the DL3DV dataset. This research is partially supported by NSERC Discovery Grants, NSERC Alliance Grants, and John R. Evans Leaders Fund (JELF). We thank the Digital Research Alliance of Canada and BC DRI Group for providing computational resources.
  </div>
</section>

<section class="section" id="References">
  <div class="container is-max-desktop content">
        <h2 class="title">References</h2>
        <div class="content has-text-justified">
          <ul>
            <li>
              <a href="https://haian-jin.github.io/projects/LVSM/" target="_blank">LVSM: A Large View Synthesis Model with Minimal 3D Inductive Bias</a>
            </li>
            <li>
              <a href="https://haofeixu.github.io/depthsplat/" target="_blank">DepthSplat: Connecting Gaussian Splatting and Depth</a>
            </li>
          </ul>
        </div>
      </div>
</section>


<footer class="footer">
  <div class="container">
    <div class="content has-text-centered">
    </div>
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website is licensed under a <a rel="license" href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
            This webpage template is adapted from <a href="https://niujinshuchong.github.io/mip-splatting/">Mip-Splatting</a>, which is originally from <a href="https://github.com/nerfies/nerfies.github.io">Nerfies</a>. 
          </p>
        </div>
      </div>
          <p></p>
        </div>
      </div>
</footer>

<script>
  // Mobile video compatibility fix for WeChat and other strict browsers
  function initializeVideoCompatibility() {
      const videos = document.querySelectorAll('video');
      let hasUserInteracted = false;
      
      videos.forEach(video => {
          // Track video state to prevent unnecessary play attempts
          let hasAttemptedPlay = false;
          let isPlayingSuccessfully = false;
          
          // Force video properties for mobile compatibility
          video.setAttribute('playsinline', '');
          video.setAttribute('webkit-playsinline', '');
          video.setAttribute('x-webkit-airplay', 'allow');
          video.setAttribute('x5-video-player-type', 'h5');
          video.setAttribute('x5-video-player-fullscreen', 'false');
          video.setAttribute('x5-video-orientation', 'portraint');
          
          // Safe play attempt that won't restart playing videos
          const attemptPlay = () => {
              // Don't attempt play if already playing successfully
              if (isPlayingSuccessfully || (!video.paused && !video.ended)) {
                  return;
              }
              
              if (video.paused && !hasAttemptedPlay) {
                  hasAttemptedPlay = true;
                  const playPromise = video.play();
                  if (playPromise !== undefined) {
                      playPromise.then(() => {
                          isPlayingSuccessfully = true;
                          console.log('Video started playing successfully');
                      }).catch(error => {
                          console.log('Autoplay prevented for video, will try on user interaction...', error);
                          hasAttemptedPlay = false; // Allow retry on user interaction
                      });
                  }
              }
          };
          
          // Track when video actually starts playing
          video.addEventListener('play', () => {
              isPlayingSuccessfully = true;
          });
          
          // Track when video stops playing
          video.addEventListener('pause', () => {
              isPlayingSuccessfully = false;
          });
          
          video.addEventListener('ended', () => {
              isPlayingSuccessfully = false;
              hasAttemptedPlay = false; // Allow replay
          });
          
          // Multiple event listeners for different loading states
          video.addEventListener('loadedmetadata', attemptPlay);
          video.addEventListener('canplay', attemptPlay);
          
          // Add click handler directly to each video
          const videoClickHandler = () => {
              if (video.paused) {
                  const playPromise = video.play();
                  if (playPromise !== undefined) {
                      playPromise.catch(() => {
                          // Play was prevented, let's show controls as a fallback.
                          if (!video.controls) {
                              video.controls = true;
                          }
                      });
                  }
              } else {
                  video.pause();
              }
          };
          
          video.addEventListener('click', videoClickHandler);
          
          // Intersection Observer for lazy loading/playing (only once)
          if ('IntersectionObserver' in window) {
              let hasObserved = false;
              const observer = new IntersectionObserver((entries) => {
                  entries.forEach(entry => {
                      if (entry.isIntersecting && !hasObserved) {
                          hasObserved = true;
                          attemptPlay();
                          observer.unobserve(entry.target);
                      }
                  });
              }, { threshold: 0.1 });
              
              observer.observe(video);
          }
      });
      
      // Global user interaction handler for strict browsers (like WeChat)
      const handleFirstUserInteraction = () => {
          if (!hasUserInteracted) {
              hasUserInteracted = true;
              console.log('First user interaction detected, attempting to play paused videos...');
              
              // Only try to play videos that are actually paused
              videos.forEach(video => {
                  if (video.paused && !video.ended) {
                      video.play().catch(error => {
                          console.log('Video autoplay still prevented after user interaction, showing controls...', error);
                          video.controls = true;
                      });
                  }
              });
              
              // Remove the listeners after first interaction
              document.removeEventListener('touchstart', handleFirstUserInteraction);
              document.removeEventListener('click', handleFirstUserInteraction);
          }
      };
      
      // Add document-wide listeners for first user interaction (for strict browsers)
      document.addEventListener('touchstart', handleFirstUserInteraction, { once: true });
      document.addEventListener('click', handleFirstUserInteraction, { once: true });
  }

// Initialize video compatibility on page load
document.addEventListener('DOMContentLoaded', initializeVideoCompatibility);
window.addEventListener('load', initializeVideoCompatibility);
</script>

</body>
</html>
